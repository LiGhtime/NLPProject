{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final COPA Project: RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>choice1</th>\n",
       "      <th>choice2</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My body cast a shadow over the grass.</td>\n",
       "      <td>The sun was rising.</td>\n",
       "      <td>The grass was cut.</td>\n",
       "      <td>cause</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The woman tolerated her friend's difficult beh...</td>\n",
       "      <td>The woman knew her friend was going through a ...</td>\n",
       "      <td>The woman felt that her friend took advantage ...</td>\n",
       "      <td>cause</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The women met for coffee.</td>\n",
       "      <td>The cafe reopened in a new location.</td>\n",
       "      <td>They wanted to catch up with each other.</td>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The runner wore shorts.</td>\n",
       "      <td>The forecast predicted high temperatures.</td>\n",
       "      <td>She planned to run along the beach.</td>\n",
       "      <td>cause</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The guests of the party hid behind the couch.</td>\n",
       "      <td>It was a surprise party.</td>\n",
       "      <td>It was a birthday party.</td>\n",
       "      <td>cause</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0              My body cast a shadow over the grass.   \n",
       "1  The woman tolerated her friend's difficult beh...   \n",
       "2                          The women met for coffee.   \n",
       "3                            The runner wore shorts.   \n",
       "4      The guests of the party hid behind the couch.   \n",
       "\n",
       "                                             choice1  \\\n",
       "0                                The sun was rising.   \n",
       "1  The woman knew her friend was going through a ...   \n",
       "2               The cafe reopened in a new location.   \n",
       "3          The forecast predicted high temperatures.   \n",
       "4                           It was a surprise party.   \n",
       "\n",
       "                                             choice2 question  label  idx  \n",
       "0                                 The grass was cut.    cause      0    0  \n",
       "1  The woman felt that her friend took advantage ...    cause      0    1  \n",
       "2           They wanted to catch up with each other.    cause      1    2  \n",
       "3                She planned to run along the beach.    cause      0    3  \n",
       "4                           It was a birthday party.    cause      0    4  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json('COPA/train.jsonl', lines=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    raw_data = pd.read_json(filename, lines=True)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    dataset = []\n",
    "    \n",
    "    for i, row in raw_data.iterrows(): # yeah, yeah, I know...\n",
    "        prem = torch.tensor(tokenizer(row['premise'])['input_ids'])\n",
    "        h1 = torch.tensor(tokenizer(row['choice1'])['input_ids'])\n",
    "        h2 = torch.tensor(tokenizer(row['choice2'])['input_ids'])\n",
    "        dataset.append((torch.cat((prem, h1, h2)), torch.tensor(row['label']).to(torch.float32)))\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded (length 350)\n",
      "Dev data loaded (length 50)\n",
      "Test data loaded (length 100)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_data('COPA/train.jsonl')\n",
    "print(f'Training data loaded (length {len(train_data)})')\n",
    "dev_data = load_data('COPA/dev.jsonl')\n",
    "print(f'Dev data loaded (length {len(dev_data)})')\n",
    "test_data = load_data('COPA/test.jsonl')\n",
    "print(f'Test data loaded (length {len(test_data)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0,   133, 32551,   376,    66,     9,     5,  6399,     4,     2,\n",
       "             0,   100, 38471,     5,  6399,     4,     2,     0,   100, 13819,\n",
       "         12552,     5,  6399,     4,     2]),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COPAClassifier(nn.Module):\n",
    "    def __init__(self, output_size=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # default pretrained layer\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "#         self.roberta.requires_grad = False\n",
    "        self.classifier = nn.Linear(768, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.roberta(input_ids=x)[1])\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return 1 if self.sigmoid(self.forward(x)) > 0.5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "copa = COPAClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Epoch: 1 ###\n",
      "tensor(50.3668, grad_fn=<DivBackward0>)\n",
      "Training completed in 0:08:59.028229\n"
     ]
    }
   ],
   "source": [
    "# 3) Now we train our model. \n",
    "start_time = datetime.now()\n",
    "epochs = 1\n",
    "bce = nn.BCELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "optimizer = optim.Adam(copa.parameters(), lr=0.05)\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('### Epoch: ' + str(i+1) + ' ###')\n",
    "    av_loss = 0\n",
    "    copa.train()\n",
    "    for (x, y) in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # a) calculate probs / get an output\n",
    "        out = copa(x.unsqueeze(0))\n",
    "        y_hat = sigmoid(out.squeeze(0).squeeze(0))\n",
    "        \n",
    "        # b) compute loss\n",
    "        loss = bce(y_hat, y)\n",
    "        av_loss += loss\n",
    "        \n",
    "        # c) get the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # d) update the weights\n",
    "        optimizer.step()\n",
    "    print(av_loss/len(train_data))\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f'Training completed in {str(end_time - start_time)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49142857142857144\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for (x, y) in train_data:\n",
    "    y_hat = copa.predict(x.unsqueeze(0))\n",
    "    if y_hat == y:\n",
    "        correct += 1\n",
    "\n",
    "acc = correct / len(train_data)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "LSTM on top of BERT is probably a bad idea - overlaps; might be better to just use a fully connected linear layer  \n",
    "Cause/effect token as extra input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
