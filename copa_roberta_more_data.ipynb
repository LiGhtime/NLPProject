{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "# import json\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import transformers\n",
    "from transformers import RobertaModel, RobertaTokenizer, RobertaForMultipleChoice\n",
    "from torch import cuda\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>asks-for</th>\n",
       "      <th>most-plausible-alternative</th>\n",
       "      <th>p</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>My body cast a shadow over the grass.</td>\n",
       "      <td>The sun was rising.</td>\n",
       "      <td>The grass was cut.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>The woman tolerated her friend's difficult beh...</td>\n",
       "      <td>The woman knew her friend was going through a ...</td>\n",
       "      <td>The woman felt that her friend took advantage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>cause</td>\n",
       "      <td>2</td>\n",
       "      <td>The women met for coffee.</td>\n",
       "      <td>The cafe reopened in a new location.</td>\n",
       "      <td>They wanted to catch up with each other.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>The runner wore shorts.</td>\n",
       "      <td>The forecast predicted high temperatures.</td>\n",
       "      <td>She planned to run along the beach.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>The guests of the party hid behind the couch.</td>\n",
       "      <td>It was a surprise party.</td>\n",
       "      <td>It was a birthday party.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>cause</td>\n",
       "      <td>2</td>\n",
       "      <td>The politician lost the election.</td>\n",
       "      <td>He ran negative campaign ads.</td>\n",
       "      <td>No one voted for him.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>cause</td>\n",
       "      <td>2</td>\n",
       "      <td>The stain came out of the shirt.</td>\n",
       "      <td>I patched the shirt.</td>\n",
       "      <td>I bleached the shirt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>cause</td>\n",
       "      <td>2</td>\n",
       "      <td>The man got a discount on his groceries.</td>\n",
       "      <td>He greeted the cashier.</td>\n",
       "      <td>He used a coupon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>effect</td>\n",
       "      <td>1</td>\n",
       "      <td>The physician misdiagnosed the patient.</td>\n",
       "      <td>The patient filed a malpractice lawsuit agains...</td>\n",
       "      <td>The patient disclosed confidential information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>cause</td>\n",
       "      <td>2</td>\n",
       "      <td>The customer filed a complaint with the store ...</td>\n",
       "      <td>The sales associate undercharged the customer.</td>\n",
       "      <td>The sales associate acted rude to the customer.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id asks-for  most-plausible-alternative  \\\n",
       "0   1    cause                           1   \n",
       "1   2    cause                           1   \n",
       "2   3    cause                           2   \n",
       "3   4    cause                           1   \n",
       "4   5    cause                           1   \n",
       "5   6    cause                           2   \n",
       "6   7    cause                           2   \n",
       "7   8    cause                           2   \n",
       "8   9   effect                           1   \n",
       "9  10    cause                           2   \n",
       "\n",
       "                                                   p  \\\n",
       "0              My body cast a shadow over the grass.   \n",
       "1  The woman tolerated her friend's difficult beh...   \n",
       "2                          The women met for coffee.   \n",
       "3                            The runner wore shorts.   \n",
       "4      The guests of the party hid behind the couch.   \n",
       "5                  The politician lost the election.   \n",
       "6                   The stain came out of the shirt.   \n",
       "7           The man got a discount on his groceries.   \n",
       "8            The physician misdiagnosed the patient.   \n",
       "9  The customer filed a complaint with the store ...   \n",
       "\n",
       "                                                  a1  \\\n",
       "0                                The sun was rising.   \n",
       "1  The woman knew her friend was going through a ...   \n",
       "2               The cafe reopened in a new location.   \n",
       "3          The forecast predicted high temperatures.   \n",
       "4                           It was a surprise party.   \n",
       "5                      He ran negative campaign ads.   \n",
       "6                               I patched the shirt.   \n",
       "7                            He greeted the cashier.   \n",
       "8  The patient filed a malpractice lawsuit agains...   \n",
       "9     The sales associate undercharged the customer.   \n",
       "\n",
       "                                                  a2  \n",
       "0                                 The grass was cut.  \n",
       "1  The woman felt that her friend took advantage ...  \n",
       "2           They wanted to catch up with each other.  \n",
       "3                She planned to run along the beach.  \n",
       "4                           It was a birthday party.  \n",
       "5                              No one voted for him.  \n",
       "6                              I bleached the shirt.  \n",
       "7                                  He used a coupon.  \n",
       "8  The patient disclosed confidential information...  \n",
       "9    The sales associate acted rude to the customer.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "test_raw_data = pd.read_xml('data/COPA-resources/datasets/copa-test.xml')\n",
    "dev_raw_data = pd.read_xml('data/COPA-resources/datasets/copa-dev.xml') # train-test-split 400-100\n",
    "dev_raw_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_sequence is:  {cause}The frozen food thawed.\n",
      "{'input_ids': [0, 45152, 27037, 24303, 133, 9214, 689, 3553, 32211, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['{', 'cause', '}', 'The', 'Ġfrozen', 'Ġfood', 'Ġth', 'awed', '.']\n",
      "test_sequence is:  {effect}I emptied my pockets.\n",
      "{'input_ids': [0, 45152, 26715, 24303, 100, 35371, 127, 12189, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['{', 'effect', '}', 'I', 'Ġemptied', 'Ġmy', 'Ġpockets', '.']\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "# test_sequence = \"{\" + \"effect\" + \"}\" + \"I ran the ice cube under warm water.\"\n",
    "test_sequence = \"{\"+ test_raw_data.iloc[28]['asks-for'] + \"}\" + test_raw_data.iloc[28]['p']\n",
    "print(\"test_sequence is: \", test_sequence)\n",
    "print(tokenizer(test_sequence))\n",
    "print(tokenizer.tokenize(test_sequence))\n",
    "# test 2\n",
    "test_sequence = \"{\"+ test_raw_data.iloc[1]['asks-for'] + \"}\" + test_raw_data.iloc[1]['p']\n",
    "print(\"test_sequence is: \", test_sequence)\n",
    "print(tokenizer(test_sequence))\n",
    "print(tokenizer.tokenize(test_sequence))\n",
    "\n",
    "print(test_raw_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(rawdata):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    \n",
    "    # for i in range(0, rawdata.shape[0]):\n",
    "    for i in range(2, 5):\n",
    "        prompt = rawdata.iloc[i]['asks-for'] + \".\" + rawdata.iloc[i]['p']\n",
    "        choice0 = rawdata.iloc[i]['a1']\n",
    "        choice1 = rawdata.iloc[i]['a2']\n",
    "        label = torch.tensor(rawdata.iloc[i]['most-plausible-alternative'] - 1)\n",
    "        # label = torch.tensor(rawdata.iloc[i]['label']).unsqueeze(0).to(device)\n",
    "\n",
    "        encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors='pt', padding=True)\n",
    "        print(\"encoding['input_ids']: \", encoding['input_ids'])\n",
    "        print(\"encoding['input_ids'] with size of : \", encoding['input_ids'].size())\n",
    "        print(\"encoding['attention_mask']: \", encoding['attention_mask'])\n",
    "        print(\"label: \", label)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(dev_raw_data.shape[0])\n",
    "print(test_raw_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding['input_ids']:  tensor([[    0, 27037,     4,   133,   390,  1145,    13,  3895,     4,     2,\n",
      "             2,   133, 16381, 14015,    11,    10,    92,  2259,     4,     2,\n",
      "             1],\n",
      "        [    0, 27037,     4,   133,   390,  1145,    13,  3895,     4,     2,\n",
      "             2,  1213,   770,     7,  2916,    62,    19,   349,    97,     4,\n",
      "             2]])\n",
      "encoding['input_ids'] with size of :  torch.Size([2, 21])\n",
      "encoding['attention_mask']:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "label:  tensor(1)\n",
      "encoding['input_ids']:  tensor([[    0, 27037,     4,   133,  7449,  5328, 13344,     4,     2,     2,\n",
      "           133,  1914,  6126,   239,  3971,     4,     2,     1,     1],\n",
      "        [    0, 27037,     4,   133,  7449,  5328, 13344,     4,     2,     2,\n",
      "          2515,  1904,     7,   422,   552,     5,  4105,     4,     2]])\n",
      "encoding['input_ids'] with size of :  torch.Size([2, 19])\n",
      "encoding['attention_mask']:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "label:  tensor(0)\n",
      "encoding['input_ids']:  tensor([[    0, 27037,     4,   133,  3958,     9,     5,   537, 20119,   639,\n",
      "             5, 16433,     4,     2,     2,   243,    21,    10,  2755,   537,\n",
      "             4,     2],\n",
      "        [    0, 27037,     4,   133,  3958,     9,     5,   537, 20119,   639,\n",
      "             5, 16433,     4,     2,     2,   243,    21,    10,  4115,   537,\n",
      "             4,     2]])\n",
      "encoding['input_ids'] with size of :  torch.Size([2, 22])\n",
      "encoding['attention_mask']:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "label:  tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# tokenize data tests\n",
    "dev_data = load_data(dev_raw_data)\n",
    "# print(f'Training data loaded (length {len(train_data)})')\n",
    "# dev_data = load_data('data/dev.jsonl')\n",
    "# print(f'Dev data loaded (length {len(dev_data)})')\n",
    "# test_data = load_data('data/test.jsonl')\n",
    "# print(f'Test data loaded (length {len(test_data)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_3, use only the very last hidden layer from Roberta.\n",
    "from torch import nn\n",
    "from transformers import RobertaConfig, RobertaModel\n",
    "\n",
    "class OurRobertaCOPA(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OurRobertaCOPA, self).__init__()\n",
    "        # self.configuration = RobertaConfig()\n",
    "        # self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        # self.l1 = RobertaModel(self.configuration)\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.l1.requires_grad = True\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.pre_classifier = torch.nn.Linear(768, 512)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        # self.classifier = torch.nn.Linear(768, 5)\n",
    "        # hidden_dim=32 for later trials.\n",
    "        # self.lstm = nn.LSTM(768, 32, 1, bias=False)\n",
    "        self.output_layer = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, sequence_1, sequence_2):\n",
    "        # Two input here\n",
    "        token_1 = tokenizer(sequence_1)\n",
    "        token_2 = tokenizer(sequence_2)\n",
    "        output_1 = self.l1(input_ids=torch.tensor(token_1[\"input_ids\"]).unsqueeze(0), attention_mask=torch.tensor(token_1[\"attention_mask\"]).unsqueeze(0))[0]\n",
    "        output_2 = self.l1(input_ids=torch.tensor(token_2[\"input_ids\"]).unsqueeze(0), attention_mask=torch.tensor(token_2[\"attention_mask\"]).unsqueeze(0))[0]\n",
    "        # RobertaModel(RobertaConfig())\n",
    "\n",
    "        # _, (hidden_rep_1, _) = self.lstm(output_1.unsqueeze(0))\n",
    "        # _, (hidden_rep_2, _) = self.lstm(output_2.unsqueeze(0))\n",
    "        # _, (hidden_rep_1, _) = self.lstm(output_1)\n",
    "        # _, (hidden_rep_2, _) = self.lstm(output_2)\n",
    "\n",
    "        hidden_rep_1 = torch.nn.ReLU()(self.pre_classifier(output_1[0])).squeeze(0)\n",
    "        hidden_rep_2 = torch.nn.ReLU()(self.pre_classifier(output_2[0])).squeeze(0)\n",
    "        pooler_1 = hidden_rep_1[:, 0]\n",
    "        pooler_2 = hidden_rep_2[:, 0]\n",
    "        # hidden_rep_1 = self.pre_classifier(output_1[0]).squeeze(0)\n",
    "        # hidden_rep_2 = self.pre_classifier(output_2[0]).squeeze(0)\n",
    "        # print(\"-------hidden_rep_1:\")\n",
    "        # print(hidden_rep_1)\n",
    "        # print(hidden_rep_1.size())\n",
    "        # print(\"-------hidden_rep_2:\")\n",
    "        # print(hidden_rep_2)\n",
    "        # print(hidden_rep_2.size())\n",
    "        \n",
    "        # hidden_rep = torch.cat((hidden_rep_1.unsqueeze(1), hidden_rep_2.unsqueeze(1)), 1)\n",
    "        # hidden_rep = self.dropout(torch.cat((hidden_rep_1, hidden_rep_2), 0))\n",
    "        hidden_rep = self.dropout(torch.cat((pooler_1, pooler_2), 0))\n",
    "\n",
    "        print(\"-------hidden_rep:\")\n",
    "        # print(hidden_rep)\n",
    "        print(hidden_rep.size())\n",
    "\n",
    "        output = self.output_layer(hidden_rep.unsqueeze(0))\n",
    "        print(\"-------output:\")\n",
    "        # print(output)\n",
    "        print(output.size())\n",
    "        print(\"--------------\")\n",
    "\n",
    "        output_squezzed = output.squeeze(0).squeeze(0)\n",
    "        print(\"-------output_squezzed:\")\n",
    "        print(output_squezzed)\n",
    "        print(output_squezzed.size())\n",
    "        print(\"--------------\")\n",
    "        \n",
    "        # y_hat = softmax(output_squezzed)\n",
    "        # y_sum =  torch.sum(y_hat, 0)\n",
    "        # col1= torch.sum(y_hat, 0)[0]\n",
    "        # col2 = torch.sum(y_hat, 0)[1]\n",
    "        # y_result = torch.tensor(torch.argmax(y_sum)).type(torch.FloatTensor)\n",
    "        # y_result = torch.tensor(y_sum)\n",
    "        \n",
    "        return output_squezzed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForMultipleChoice: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForMultipleChoice(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialization\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# model = OurRobertaCOPA()\n",
    "model = RobertaForMultipleChoice.from_pretrained('roberta-base')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Epoch: 1--------------\n",
      "Training for epoch 1.......\n",
      "train_loss:  tensor(1.0145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-7.2321, -6.6679]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(2.1292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-7.3377, -5.3351]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  1.093465805053711\n",
      ".......Validating for epoch 1\n",
      "dev_loss:  tensor(0.2771, device='cuda:0')\n",
      "dev_logits:  tensor([[-4.4653, -3.3236]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  49.0\n",
      "dev_acc[j]:  50.0\n",
      "dev_acc[j]:  50.0\n",
      "dev_acc[j]:  51.0\n",
      "dev_acc[j]:  52.0\n",
      "dev_acc[j]:  52.0\n",
      "dev_acc[j]:  53.0\n",
      "dev_acc[j]:  53.0\n",
      "dev_acc[j]:  54.0\n",
      "dev_loss:  tensor(0.9445, device='cuda:0')\n",
      "dev_logits:  tensor([[-5.1815, -5.6335]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  54.0\n",
      "--------------Epoch: 2--------------\n",
      "Training for epoch 2.......\n",
      "train_loss:  tensor(0.7258, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-4.7509, -4.6866]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-4.9796, -7.7781]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  1.1721071004867554\n",
      ".......Validating for epoch 2\n",
      "dev_loss:  tensor(2.0170, device='cuda:0')\n",
      "dev_logits:  tensor([[-4.6715, -6.5457]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  49.0\n",
      "dev_acc[j]:  49.0\n",
      "dev_acc[j]:  49.0\n",
      "dev_loss:  tensor(0.0664, device='cuda:0')\n",
      "dev_logits:  tensor([[-6.1951, -3.5165]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  50.0\n",
      "--------------Epoch: 3--------------\n",
      "Training for epoch 3.......\n",
      "train_loss:  tensor(2.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-7.4392, -4.8230]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-4.3327, -7.9884]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  1.2429717779159546\n",
      ".......Validating for epoch 3\n",
      "dev_loss:  tensor(1.0223, device='cuda:0')\n",
      "dev_logits:  tensor([[-1.0750, -1.6513]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  49.0\n",
      "dev_acc[j]:  50.0\n",
      "dev_acc[j]:  51.0\n",
      "dev_loss:  tensor(0.3926, device='cuda:0')\n",
      "dev_logits:  tensor([[-2.3238, -1.5917]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  52.0\n",
      "--------------Epoch: 4--------------\n",
      "Training for epoch 4.......\n",
      "train_loss:  tensor(3.3987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-3.5012, -0.1366]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.3746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[ 0.1935, -0.5953]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  1.2903330326080322\n",
      ".......Validating for epoch 4\n",
      "dev_loss:  tensor(1.0326, device='cuda:0')\n",
      "dev_logits:  tensor([[-5.1778, -5.7702]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_loss:  tensor(0.0530, device='cuda:0')\n",
      "dev_logits:  tensor([[-6.6206, -3.7099]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  48.0\n",
      "--------------Epoch: 5--------------\n",
      "Training for epoch 5.......\n",
      "train_loss:  tensor(3.7577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-7.0622, -3.3281]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.6622, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-2.9537, -3.0167]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  1.2250010967254639\n",
      ".......Validating for epoch 5\n",
      "dev_loss:  tensor(0.1163, device='cuda:0')\n",
      "dev_logits:  tensor([[-3.9811, -1.8886]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_loss:  tensor(0.0354, device='cuda:0')\n",
      "dev_logits:  tensor([[-6.6629, -3.3401]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  43.0\n",
      "--------------Epoch: 6--------------\n",
      "Training for epoch 6.......\n",
      "train_loss:  tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-3.4060, -5.3394]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-5.1232, -7.3241]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  1.15818190574646\n",
      ".......Validating for epoch 6\n",
      "dev_loss:  tensor(2.4914, device='cuda:0')\n",
      "dev_logits:  tensor([[ -9.5947, -11.9997]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_loss:  tensor(0.0263, device='cuda:0')\n",
      "dev_logits:  tensor([[-10.2586,  -6.6338]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  43.0\n",
      "--------------Epoch: 7--------------\n",
      "Training for epoch 7.......\n",
      "train_loss:  tensor(3.8707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-8.2238, -4.3742]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(1.9572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-8.5520, -6.7470]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  1.173902988433838\n",
      ".......Validating for epoch 7\n",
      "dev_loss:  tensor(1.2510, device='cuda:0')\n",
      "dev_logits:  tensor([[-5.4386, -6.3524]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_loss:  tensor(2.2884, device='cuda:0')\n",
      "dev_logits:  tensor([[-4.6569, -6.8383]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  47.0\n",
      "--------------Epoch: 8--------------\n",
      "Training for epoch 8.......\n",
      "train_loss:  tensor(1.2080, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-7.5913, -6.7383]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.3395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-7.2029, -8.1088]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  1.1467746496200562\n",
      ".......Validating for epoch 8\n",
      "dev_loss:  tensor(0.0329, device='cuda:0')\n",
      "dev_logits:  tensor([[-9.4285, -6.0299]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_loss:  tensor(0.6732, device='cuda:0')\n",
      "dev_logits:  tensor([[-7.3134, -7.2731]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  49.0\n",
      "--------------Epoch: 9--------------\n",
      "Training for epoch 9.......\n",
      "train_loss:  tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-5.6943, -6.7532]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.5174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-7.9101, -8.2991]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  1.0407772064208984\n",
      ".......Validating for epoch 9\n",
      "dev_loss:  tensor(1.1727, device='cuda:0')\n",
      "dev_logits:  tensor([[-8.1714, -8.9736]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  49.0\n",
      "dev_acc[j]:  49.0\n",
      "dev_loss:  tensor(1.2766, device='cuda:0')\n",
      "dev_logits:  tensor([[-7.0350, -7.9844]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  49.0\n",
      "--------------Epoch: 10--------------\n",
      "Training for epoch 10.......\n",
      "train_loss:  tensor(0.6376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-5.9890, -6.1034]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.1934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-7.5469, -9.0916]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  0.9895613789558411\n",
      ".......Validating for epoch 10\n",
      "dev_loss:  tensor(0.3310, device='cuda:0')\n",
      "dev_logits:  tensor([[-6.1371, -5.2016]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_loss:  tensor(0.0376, device='cuda:0')\n",
      "dev_logits:  tensor([[-7.7408, -4.4785]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  48.0\n",
      "--------------Epoch: 11--------------\n",
      "Training for epoch 11.......\n",
      "train_loss:  tensor(0.5812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-6.1057, -6.3437]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(1.4295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-8.7222, -7.5664]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  0.9894933700561523\n",
      ".......Validating for epoch 11\n",
      "dev_loss:  tensor(0.4250, device='cuda:0')\n",
      "dev_logits:  tensor([[-6.6095, -5.9738]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_loss:  tensor(0.9675, device='cuda:0')\n",
      "dev_logits:  tensor([[-6.8983, -7.3878]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  48.0\n",
      "--------------Epoch: 12--------------\n",
      "Training for epoch 12.......\n",
      "train_loss:  tensor(1.9728, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-7.1491, -5.3261]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.8282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-5.4109, -5.1569]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  0.9458529949188232\n",
      ".......Validating for epoch 12\n",
      "dev_loss:  tensor(0.2779, device='cuda:0')\n",
      "dev_logits:  tensor([[-6.2402, -5.1019]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  49.0\n",
      "dev_acc[j]:  50.0\n",
      "dev_loss:  tensor(0.0366, device='cuda:0')\n",
      "dev_logits:  tensor([[-8.2777, -4.9896]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  51.0\n",
      "--------------Epoch: 13--------------\n",
      "Training for epoch 13.......\n",
      "train_loss:  tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-5.6029, -6.8350]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.3392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-4.9134, -5.8201]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  0.9291983842849731\n",
      ".......Validating for epoch 13\n",
      "dev_loss:  tensor(1.0596, device='cuda:0')\n",
      "dev_logits:  tensor([[-7.4861, -8.1201]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  49.0\n",
      "dev_acc[j]:  49.0\n",
      "dev_acc[j]:  49.0\n",
      "dev_acc[j]:  50.0\n",
      "dev_acc[j]:  50.0\n",
      "dev_acc[j]:  50.0\n",
      "dev_loss:  tensor(0.6701, device='cuda:0')\n",
      "dev_logits:  tensor([[-8.2852, -8.2386]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  51.0\n",
      "--------------Epoch: 14--------------\n",
      "Training for epoch 14.......\n",
      "train_loss:  tensor(4.4508, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-10.3076,  -5.8686]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(1.2514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-9.2243, -8.3100]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  0.9405280351638794\n",
      ".......Validating for epoch 14\n",
      "dev_loss:  tensor(0.3756, device='cuda:0')\n",
      "dev_logits:  tensor([[-8.6005, -7.8151]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  49.0\n",
      "dev_acc[j]:  50.0\n",
      "dev_acc[j]:  50.0\n",
      "dev_acc[j]:  50.0\n",
      "dev_acc[j]:  50.0\n",
      "dev_acc[j]:  50.0\n",
      "dev_acc[j]:  51.0\n",
      "dev_acc[j]:  52.0\n",
      "dev_acc[j]:  52.0\n",
      "dev_acc[j]:  53.0\n",
      "dev_loss:  tensor(0.6886, device='cuda:0')\n",
      "dev_logits:  tensor([[-10.1946, -10.1856]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  54.0\n",
      "--------------Epoch: 15--------------\n",
      "Training for epoch 15.......\n",
      "train_loss:  tensor(1.3952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-10.6107,  -9.5002]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.2591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[ -8.8253, -10.0435]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  0.9213146567344666\n",
      ".......Validating for epoch 15\n",
      "dev_loss:  tensor(1.3565, device='cuda:0')\n",
      "dev_logits:  tensor([[-8.2438, -9.3025]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_loss:  tensor(0.1561, device='cuda:0')\n",
      "dev_logits:  tensor([[-7.8347, -6.0565]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  46.0\n",
      "--------------Epoch: 16--------------\n",
      "Training for epoch 16.......\n",
      "train_loss:  tensor(1.3832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-8.7077, -7.6132]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[ -8.5922, -10.1212]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  0.9412047266960144\n",
      ".......Validating for epoch 16\n",
      "dev_loss:  tensor(1.7100, device='cuda:0')\n",
      "dev_logits:  tensor([[-7.5194, -9.0299]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_loss:  tensor(0.7339, device='cuda:0')\n",
      "dev_logits:  tensor([[-7.5996, -7.6794]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  45.0\n",
      "--------------Epoch: 17--------------\n",
      "Training for epoch 17.......\n",
      "train_loss:  tensor(0.1894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-7.0297, -8.5973]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.3108, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-7.3592, -8.3685]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  0.886634349822998\n",
      ".......Validating for epoch 17\n",
      "dev_loss:  tensor(0.3563, device='cuda:0')\n",
      "dev_logits:  tensor([[-7.8554, -7.0069]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_acc[j]:  45.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  46.0\n",
      "dev_acc[j]:  47.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  48.0\n",
      "dev_acc[j]:  49.0\n",
      "dev_loss:  tensor(1.3865, device='cuda:0')\n",
      "dev_logits:  tensor([[-7.1417, -8.2406]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  49.0\n",
      "--------------Epoch: 18--------------\n",
      "Training for epoch 18.......\n",
      "train_loss:  tensor(0.9722, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-7.8583, -7.3613]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.8283, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-8.1563, -7.9022]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  0.8542928695678711\n",
      ".......Validating for epoch 18\n",
      "dev_loss:  tensor(0.0814, device='cuda:0')\n",
      "dev_logits:  tensor([[-9.0063, -6.5391]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  38.0\n",
      "dev_acc[j]:  39.0\n",
      "dev_acc[j]:  40.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  41.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  42.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  43.0\n",
      "dev_acc[j]:  44.0\n",
      "dev_loss:  tensor(0.7888, device='cuda:0')\n",
      "dev_logits:  tensor([[-8.5074, -8.6904]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  44.0\n",
      "--------------Epoch: 19--------------\n",
      "Training for epoch 19.......\n",
      "train_loss:  tensor(0.9701, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-8.2991, -7.8055]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.1920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-7.5243, -9.0771]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "av_train_loss:  0.9035339951515198\n",
      ".......Validating for epoch 19\n",
      "dev_loss:  tensor(2.7315, device='cuda:0')\n",
      "dev_logits:  tensor([[ -8.2340, -10.8982]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  0.0\n",
      "dev_acc[j]:  1.0\n",
      "dev_acc[j]:  2.0\n",
      "dev_acc[j]:  3.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  4.0\n",
      "dev_acc[j]:  5.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  6.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  7.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  8.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  9.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  10.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  11.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  12.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  13.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  14.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  15.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  16.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  17.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  18.0\n",
      "dev_acc[j]:  19.0\n",
      "dev_acc[j]:  20.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  21.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  22.0\n",
      "dev_acc[j]:  23.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  24.0\n",
      "dev_acc[j]:  25.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  26.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  27.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  28.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  29.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  30.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  31.0\n",
      "dev_acc[j]:  32.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  33.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  34.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  35.0\n",
      "dev_acc[j]:  36.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_acc[j]:  37.0\n",
      "dev_loss:  tensor(0.7426, device='cuda:0')\n",
      "dev_logits:  tensor([[-7.8421, -7.9386]], device='cuda:0')\n",
      "label:  tensor([1], device='cuda:0')\n",
      "dev_acc[j]:  37.0\n",
      "--------------Epoch: 20--------------\n",
      "Training for epoch 20.......\n",
      "train_loss:  tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[ -7.9393, -10.2256]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n",
      "train_loss:  tensor(0.7069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train_logits:  tensor([[-8.8029, -8.7757]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "label:  tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ce = nn.CrossEntropyLoss()\n",
    "softmax = nn.Softmax(dim=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "epochs = 52\n",
    "per_num_epoch = 1\n",
    "\n",
    "# train_acc = np.zeros(epochs)\n",
    "train_loss_by_epoch = np.zeros(epochs)\n",
    "dev_acc = np.zeros(epochs)\n",
    "dev_loss_by_epoch = np.zeros(epochs)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for j in range(epochs):\n",
    "    if j % per_num_epoch == 0:\n",
    "        print('--------------Epoch: ' + str(j+1) + '--------------')\n",
    "    \n",
    "    if j % per_num_epoch == 0:\n",
    "        print(f'Training for epoch {j + 1}.......')\n",
    "    \n",
    "    av_train_loss = 0\n",
    "    # print(\"av_train_loss_original: \", av_train_loss)\n",
    "    model.train()\n",
    "    for i in range(0, dev_raw_data.shape[0] - 100):\n",
    "        # print(\"av_train_loss_track: \", av_train_loss)\n",
    "        prompt = dev_raw_data.iloc[i]['asks-for'] + \". \" + dev_raw_data.iloc[i]['p']\n",
    "        choice0 = dev_raw_data.iloc[i]['a1']\n",
    "        choice1 = dev_raw_data.iloc[i]['a2']\n",
    "        label = torch.tensor(dev_raw_data.iloc[i]['most-plausible-alternative'] - 1).unsqueeze(0).to(device)\n",
    "        # print(\"label is: \", label)\n",
    "\n",
    "        encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors='pt', padding=True).to(device)\n",
    "        # encoding = {(prompt+choice0), (prompt+choice1)}\n",
    "        # outputs = model(input_ids=encoding['input_ids'].unsqueeze(0), attention_mask=encoding['attention_mask'].unsqueeze(0), labels=label)\n",
    "        outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=label)\n",
    "        # print(\"outputs: \", outputs)\n",
    "\n",
    "        train_loss = outputs.loss\n",
    "        train_logits = outputs.logits\n",
    "        av_train_loss += train_loss\n",
    "\n",
    "        if i == 0:\n",
    "            print(\"train_loss: \", train_loss)\n",
    "            print(\"train_logits: \", train_logits)\n",
    "            print(\"label: \", label)\n",
    "        if i == 1:\n",
    "            print(\"train_loss: \", train_loss)\n",
    "            print(\"train_logits: \", train_logits)\n",
    "            print(\"label: \", label)\n",
    "\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_loss_by_epoch[j] = av_train_loss / (dev_raw_data.shape[0] - 100)\n",
    "    print(\"av_train_loss: \", train_loss_by_epoch[j])\n",
    "\n",
    "    # validation\n",
    "    # if (j + 1) % per_num_epoch == 0:\n",
    "    #     print(f'.......Validating for epoch {j + 1}')\n",
    "    if (j) % per_num_epoch == 0:\n",
    "        print(f'.......Validating for epoch {j + 1}')\n",
    "        av_dev_loss = 0\n",
    "        # model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(dev_raw_data.shape[0] - 99, dev_raw_data.shape[0]):\n",
    "                # print(\"av_dev_loss_track: \", av_dev_loss)\n",
    "                prompt_val = dev_raw_data.iloc[i]['asks-for'] + \". \" + dev_raw_data.iloc[i]['p']\n",
    "                choice0_val = dev_raw_data.iloc[i]['a1']\n",
    "                choice1_val = dev_raw_data.iloc[i]['a2']\n",
    "                label_val = torch.tensor(dev_raw_data.iloc[i]['most-plausible-alternative'] - 1).unsqueeze(0).to(device)\n",
    "\n",
    "                encoding_val = tokenizer([prompt_val, prompt_val], [choice0_val, choice1_val], return_tensors='pt', padding=True).to(device)\n",
    "                # outputs = model(input_ids=encoding['input_ids'].unsqueeze(0), attention_mask=encoding['attention_mask'].unsqueeze(0), labels=label)\n",
    "                outputs_val = model(**{k: v.unsqueeze(0) for k,v in encoding_val.items()}, labels=label_val)\n",
    "                \n",
    "                dev_loss = outputs_val.loss\n",
    "                dev_logits = outputs_val.logits\n",
    "                av_dev_loss += dev_loss\n",
    "                \n",
    "                if i == dev_raw_data.shape[0] - 99:\n",
    "                    print(\"dev_loss: \", dev_loss)\n",
    "                    print(\"dev_logits: \", dev_logits)\n",
    "                    print(\"label: \", label_val)\n",
    "                if i == dev_raw_data.shape[0] - 1:\n",
    "                    print(\"dev_loss: \", dev_loss)\n",
    "                    print(\"dev_logits: \", dev_logits)\n",
    "                    print(\"label: \", label_val)\n",
    "\n",
    "                #calculate accuracy\n",
    "                y_pred = 1 if outputs_val.logits[0][1] > outputs_val.logits[0][0] else 0\n",
    "                y_pred = torch.tensor(y_pred).unsqueeze(0).to(device)\n",
    "                # print(\"y_pred: \", y_pred)\n",
    "                # print(\"label: \", label)\n",
    "                # print(\"y_pred =? label: \", y_pred == label)\n",
    "                if y_pred == label_val:\n",
    "                    dev_acc[j] += 1\n",
    "                \n",
    "        dev_acc[j] /= 100\n",
    "        print(\"dev_acc[j]: \", dev_acc[j])\n",
    "        dev_loss_by_epoch[j] = av_dev_loss / 100\n",
    "        \n",
    "    # learning rate decay\n",
    "    # if j == 5:\n",
    "    #     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    # elif j == 15:\n",
    "    #     optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    # elif j == 20:\n",
    "    #     optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    # elif j == 40:\n",
    "    #     optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "    # elif j == 50:\n",
    "    #     optimizer = torch.optim.Adam(model.parameters(), lr=1e-7)    \n",
    "    scheduler.step()\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f'Training completed in {str(end_time - start_time)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.title(\"Loss VS Epoch\")\n",
    "\n",
    "plt.plot(train_loss_by_epoch, label=\"train_loss\")\n",
    "plt.plot(dev_loss_by_epoch, label=\"dev_loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.title(\"Loss VS Epoch\")\n",
    "\n",
    "plt.plot(dev_loss_by_epoch, label=\"dev_acc\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model, 'RoBERTa.pth')# epochs = 100, lr=1e-2 -> scheular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = torch.load('RoBERTa.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.eval()\n",
    "num_correct_pred = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(dev_raw_data.shape[0] - 99, dev_raw_data.shape[0]):\n",
    "        prompt = dev_raw_data.iloc[i]['asks-for'] + \". \" + dev_raw_data.iloc[i]['p']\n",
    "        choice0 = dev_raw_data.iloc[i]['a1']\n",
    "        choice1 = dev_raw_data.iloc[i]['a2']\n",
    "        label_test = torch.tensor(dev_raw_data.iloc[i]['most-plausible-alternative']).unsqueeze(0).to(device)\n",
    "\n",
    "        encoding_test = tokenizer([prompt, prompt], [choice0, choice1], return_tensors='pt', padding=True).to(device)\n",
    "        # outputs = test_model(input_ids=encoding['input_ids'].unsqueeze(0), attention_mask=encoding['attention_mask'].unsqueeze(0), labels=label)\n",
    "        outputs_val = model(**{k: v.unsqueeze(0) for k,v in encoding_test.items()}, labels=label_test)\n",
    "        test_logits = outputs.logits\n",
    "\n",
    "        #calculate accuracy\n",
    "        y_pred_test = 1 if outputs.logits[0][1] > outputs.logits[0][0] else 0\n",
    "        y_pred_test = torch.tensor(y_pred_test).unsqueeze(0).to(device)\n",
    "\n",
    "        if y_pred_test == label_test:\n",
    "            print(\"test_logits: \", test_logits)\n",
    "            print(\"y_pred: \", y_pred_test)\n",
    "            print(\"label: \", label_test)\n",
    "            num_correct_pred += 1\n",
    "\n",
    "acc = num_correct_pred / test_raw_data.shape[0]\n",
    "print(\"test_accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_logits:  tensor([[1193.0547, 1193.0547]], device='cuda:0')\n",
      "y_pred:  tensor([0], device='cuda:0')\n",
      "label:  tensor([0], device='cuda:0')\n",
      "test_accuracy =  0.55\n"
     ]
    }
   ],
   "source": [
    "# test_model.eval()\n",
    "num_correct_pred = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, test_raw_data.shape[0]):\n",
    "        prompt = test_raw_data.iloc[i]['question'] + \". \" + test_raw_data.iloc[i]['premise']\n",
    "        choice0 = test_raw_data.iloc[i]['choice1']\n",
    "        choice1 = test_raw_data.iloc[i]['choice2']\n",
    "        label = torch.tensor(test_raw_data.iloc[i]['label']).unsqueeze(0).to(device)\n",
    "\n",
    "        encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors='pt', padding=True).to(device)\n",
    "        outputs = test_model(input_ids=encoding['input_ids'].unsqueeze(0), attention_mask=encoding['attention_mask'].unsqueeze(0), labels=label)\n",
    "\n",
    "        test_logits = outputs.logits\n",
    "\n",
    "        #calculate accuracy\n",
    "        y_pred = 1 if outputs.logits[0][1] > outputs.logits[0][0] else 0\n",
    "        y_pred = torch.tensor(y_pred).unsqueeze(0).to(device)\n",
    "\n",
    "        if y_pred == label:\n",
    "            print(\"test_logits: \", test_logits)\n",
    "            print(\"y_pred: \", y_pred)\n",
    "            print(\"label: \", label)\n",
    "            num_correct_pred += 1\n",
    "\n",
    "acc = num_correct_pred / test_raw_data.shape[0]\n",
    "print(\"test_accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revise\n",
    "# Training\n",
    "ce = nn.CrossEntropyLoss()\n",
    "softmax = nn.Softmax(dim=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "epochs = 100\n",
    "per_num_epoch = 1\n",
    "\n",
    "# train_acc = np.zeros(epochs)\n",
    "train_loss_by_epoch = np.zeros(epochs)\n",
    "dev_acc = np.zeros(epochs)\n",
    "dev_loss_by_epoch = np.zeros(epochs)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for j in range(epochs):\n",
    "    if j % per_num_epoch == 0:\n",
    "        print('--------------Epoch: ' + str(j+1) + '--------------')\n",
    "    \n",
    "    if j % per_num_epoch == 0:\n",
    "        print(f'Training for epoch {j + 1}.......')\n",
    "    av_train_loss = 0\n",
    "    # print(\"av_train_loss_original: \", av_train_loss)\n",
    "    model.train()\n",
    "    for i in range(0, train_raw_data.shape[0]):\n",
    "        # print(\"av_train_loss_track: \", av_train_loss)\n",
    "        prompt = train_raw_data.iloc[i]['question'] + \". \" + train_raw_data.iloc[i]['premise']\n",
    "        choice0 = train_raw_data.iloc[i]['choice1']\n",
    "        choice1 = train_raw_data.iloc[i]['choice2']\n",
    "        label = torch.tensor(train_raw_data.iloc[i]['label']).unsqueeze(0).to(device)\n",
    "        # print(\"label is: \", label)\n",
    "        # label = torch.tensor(rawdata.iloc[i]['label']).unsqueeze(0).to(device)\n",
    "\n",
    "        encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors='pt', padding=True).to(device)\n",
    "        # outputs = model(input_ids=encoding['input_ids'].unsqueeze(0), attention_mask=encoding['attention_mask'].unsqueeze(0), labels=label)\n",
    "        \n",
    "        outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=label)\n",
    "        \n",
    "        train_loss = outputs.loss\n",
    "        train_logits = outputs.logits\n",
    "        av_train_loss += train_loss\n",
    "\n",
    "        if i == 0:\n",
    "            print(\"train_loss: \", train_loss)\n",
    "            print(\"train_logits: \", train_logits)\n",
    "            print(\"label: \", label)\n",
    "        if i == 1:\n",
    "            print(\"train_loss: \", train_loss)\n",
    "            print(\"train_logits: \", train_logits)\n",
    "            print(\"label: \", label)\n",
    "\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # learning rate decay\n",
    "        if j == 25:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "        elif j == 50:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-7)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_loss_by_epoch[j] = av_train_loss / train_raw_data.shape[0]\n",
    "    print(\"av_train_loss: \", train_loss_by_epoch[j])\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f'Training completed in {str(end_time - start_time)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES\n",
    "1.Ask about whether the very last output of RoBERTaMultipleChoice is the possibility score for one input embedding.\n",
    "\n",
    "(pooler): RobertaPooler(\n",
    "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "      (activation): Tanh()\n",
    "    )\n",
    "  )\n",
    "  (dropout): Dropout(p=0.1, inplace=False)\n",
    "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
    "\n",
    "2. What's wrong with the model.eval()?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a411169b02b4f5d985b282c4f140db7a58e4cacae7bbcf8f29fd937be3ae09c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('NLP_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
